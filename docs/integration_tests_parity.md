# Integration Test Parity Plan

_Last updated: 2025-11-12_

Ultramarine currently ships fast, deterministic blob-focused integration tests that exercise
`State<TestBlobEngine>` with real RocksDB stores and KZG verification. These scenarios caught
multiple production bugs, but they terminate at the consensus state boundary: libp2p gossip,
Malachite channel actors, WAL, timers, and the execution bridge are replaced with lightweight
helpers or mocks.

Both upstream projects we claim parity with already run multi-validator,
networked harnesses exclusively:

- **Malachite** always brings up at least three validators plus follower nodes inside the
  TestBuilder harness; even the ‚Äúbasic‚Äù scenario drives 3 validators and 2 followers to height‚ÄØ5
  and later tests mix crash/restart + sync behaviour (`malachite/code/crates/test/tests/it/full_nodes.rs:11-175`).
- **Snapchain** follows the same pattern: the consensus test harness wires libp2p gossip,
  gRPC services, RocksDB stores, and multiple nodes with deterministic port allocation before any
  assertions run (`snapchain/tests/consensus_test.rs:1-370`), and the suite is serialized via
  `serial_test` to avoid cross-talk.

Their approach means ‚ÄúTier‚ÄØ1‚Äù coverage is inherently multi-node: leader election, quorum votes,
ValueSync, crash/restart, and networking are all exercised together. This document tracks the work
required to reach the same architecture instead of relying on single-node smoke tests.

This document tracks the work required to reach similar parity. The goal is to **keep** the
existing state-level tests as Tier¬†0 regression coverage and **add** a second tier that boots
real Ultramarine nodes and drives blob scenarios through `/proposal_parts` gossip, WAL, and the
execution bridge.

**Terminology**: The execution-layer (EL) bridge refers to the production `ExecutionClient`
(`crates/execution`) plus the Engine API calls (`forkchoiceUpdatedV3`, `getPayloadV3`,
`newPayloadV3`, blob bundles) that tie Malachite's Tendermint-style consensus actors to the EL.
Whenever this plan says ‚Äúwire the EL bridge,‚Äù it means exercising that exact HTTP/IPC surface
instead of calling `MockEngineApi` directly.

---

## 1. Current Coverage Snapshot

| Layer / Component                    | Exercised Today? | Notes |
|-------------------------------------|------------------|-------|
| Consensus `State` + BlobEngine      | ‚úÖ               | All Tier‚ÄØ0 scenarios (now under `crates/test/tests/blob_state`) run against the real state/engine with RocksDB. |
| Execution payload + blob verifier   | ‚úÖ               | Deterministic payloads, real KZG commitments/proofs via `c-kzg`. |
| Engine API bridge (generate block)  | ‚ö†Ô∏è Mocked        | [`MockEngineApi`](../crates/test/tests/common/mocks.rs) returns canned payloads/bundles; no Engine API client or EL node. |
| Execution notifier (FCU / payload)  | ‚ö†Ô∏è Mocked        | `MockExecutionNotifier` captures calls but does not touch the real ExecutionClient. |
| Malachite channel actors            | ‚ùå               | Tests bypass `AppMsg`/`NetworkMsg` and call state methods directly. |
| libp2p gossip transport             | ‚ùå               | No `/proposal_parts` streaming over the network stack. |
| WAL / timers / crash recovery paths | ‚ùå               | Not covered; restarts simulated via store reopen only. |

---

## 2. Target Parity Goals

1. **Channel-Service Harness**  
   Run at least one Ultramarine node (consensus channel actors + WAL + libp2p) in-process,
   publish a blobbed proposal via `/proposal_parts`, and verify followers commit.

2. **Multi-Validator Scenario**  
   Extend the harness to **three** validators (2f‚ÄØ+‚ÄØ1) connected over loopback libp2p, plus optional
   follower full nodes. Cover proposer/follower flow, sync recovery, and restart hydration under
   real networking/timers. Single-node tests are explicitly out of scope and will be removed.

3. **Execution Bridge Coverage**  
   Replace the current mock with the production ExecutionClient (pointed at an in-process or
   recorded Engine API stub) so `generate_block_with_blobs` and `notify_new_block` paths exercise
   the same code as a live node.

4. **Negative Paths Under Full Node**  
   Port at least the commitment-mismatch and invalid-proof scenarios to the full-node harness to
   ensure WAL cleanup + gossip error handling behave as expected.

5. **CI Integration**  
   Decide on cadence (e.g., nightly or gated job) so the heavier harness does not slow down the
   default developer loop but still catches regressions before release.

6. **(Optional) Ethereum Spec Compliance**  
   Once full-node parity lands, consider adding Deneb/Cancun-specific coverage (blobless payload
   fallback, `engine_getBlobsV1`, blob sidecar gossip APIs). Track as a future Phase P7 if needed.

---

## 2a. Migration Plan (from single-node ‚Üí multi-validator)

1. **Retire legacy single-node tests**  
   Delete `full_node_blob_roundtrip` and any single-validator helpers. Until the multi-validator
   harness lands, gate `make itest-node` (or mark the binary `#[ignore]`) so developers are not
   misled by invalid coverage.

2. **Promote the existing dual-node scaffolding to 3 validators**  
   Reuse `NetworkHarness` but always instantiate with 3 validators (and optional followers). Rename
   the surviving test to reflect blob quorum coverage and assert every validator persisted blobs.

3. **Port Tier‚ÄØ0 scenarios into the new harness**  
   Once the baseline is stable, iteratively add restart, sync, and negative-path cases on top of
   the multi-validator harness (mirroring Malachite‚Äôs `full_nodes.rs` suite) before progressing to
   P3/P4 in the work plan.

4. **Wire Execution Bridge + ValueSync**  
   After the harness runs deterministically, switch it to the Engine RPC stub + real ExecutionClient
   and enable ValueSync just like the upstream tests so blob sidecars, WAL, and sync packages are
   exercised together.

---

## 3. Phased Work Plan

| Phase | Description | Deliverables & Acceptance Criteria | Owner | Est. Effort | Depends On | Status |
|-------|-------------|------------------------------------|-------|------------|------------|--------|
| P0 | Finalize parity scope & infra decisions | This doc, shared understanding of Tier‚ÄØ0 (`blob_state/`) vs Tier‚ÄØ1 (`full_node/`), and action list. | @team | 0.5‚ÄØd | ‚Äî | ‚úÖ |
| P1 | Multi-validator harness baseline | `crates/test/tests/full_node/` boots **three** validators (2f+1) plus optional full nodes, mirroring Malachite‚Äôs TestBuilder. `make itest-node` must exercise proposer/follower votes, `/proposal_parts` gossip, and WAL checkpoints end-to-end using the Engine RPC stub. | @team | 3‚ÄØd | P0 | üü° |
| P2 | Crash/sync flows on harness | Extend the multi-validator harness with follower nodes, restarts, and ValueSync enabled. At least one validator/full-node crash-and-recover path plus a late joiner must pass deterministically. *(Engine stub persistence + resume-height logging landed; restart scenario now stable with 100% pass rate using event-based waiting and immediate node-0 shutdown to force ValueSync.)* | @team | 2‚ÄØd | P1 | ‚úÖ |
| P3 | Sync & restart cases | Port `blob_sync_across_restart_multiple_heights` and `blob_restart_hydrates_multiple_heights` into Tier‚ÄØ1. **Done when**: restart path exercises WAL/timers (not just store reopen) and passes deterministically. | TBA | 2‚ÄØd | P2 | ‚è≥ |
| P4 | Negative-path parity | Tier‚ÄØ1 versions of commitment mismatch, invalid proof, EL rejection. **Done when**: node logs/metrics show rejection, WAL cleanup verified. (Can run parallel w/ P3 once P2 is stable.) | TBA | 1‚Äì2‚ÄØd | P2 | ‚è≥ |
| P5 (optional) | Execution bridge wiring | Replace `MockEngineApi` with HTTP Engine stub so the real `ExecutionClient` path runs unmodified; later optional upgrade to real reth devnet. **Done when**: `generate_block_with_blobs`, `notify_new_block`, `forkchoice_updated` go over HTTP stub. | TBA | 2‚Äì3‚ÄØd | P2 | ‚è≥ |
| P6 (optional) | CI integration & docs | Update `DEV_WORKFLOW.md`, add `make itest-node`, decide CI cadence (manual/nightly/per-PR). **Done when**: documented instructions + optional CI job exist. | TBA | 1‚ÄØd | P2‚ÄìP5 | ‚è≥ |

---

## 4. Near-Term Action Items

| Item | Description | Owner | Priority | Status |
|------|-------------|-------|----------|--------|
| Tier‚ÄØ0 reorg | Move existing state tests into `crates/test/tests/blob_state/`, update Makefile/docs references. | @team | High | ‚úÖ |
| Harness Skeleton (P1) | Build the **multi-validator** harness using real libp2p transport + WAL, modeled after Malachite‚Äôs TestBuilder (‚â•3 validators + optional full nodes). Current single-node helper must be replaced. | @team | High | üü° |
| Scenario Porting | Tier‚ÄØ1 must cover proposer/follower, crash/restart, and sync scenarios on the multi-node harness (no single-node shortcuts). | @team | High | ‚è≥ |
| Execution Bridge Stub | HTTP Engine stub that exercises the real `ExecutionClient` (replaces [`MockEngineApi`](../crates/test/tests/common/mocks.rs)). Landed via `EngineRpcStub` in the new harness. | @team | Medium | ‚úÖ |
| Docs & CI | Keep docs in sync (`DEV_WORKFLOW.md`, this plan) and decide when `make itest-node` runs in CI. Docs landed; CI cadence pending runtime metrics from P2. | @team | Medium | üü° |

---

## 5. Open Questions & Decision Process

| Question | Current Position | Decision Owner | Target Timing |
|----------|------------------|----------------|---------------|
| Transport choice | Use real libp2p TCP (same as Malachite/Snapchain). Revisit only if flakiness becomes unmanageable. | @you | Locked for P1 |
| Execution-layer strategy | Start with HTTP Engine stub that uses real `ExecutionClient`; optional later work to run against reth devnet. | @you | Before P5 |
| Runtime/CI budget | Measure after P2; default to manual/nightly runs unless suite <1‚ÄØmin. | @you | After P2 |

Decisions are recorded here; once you approve a direction it becomes part of scope.

---

## 6. Risks & Mitigations

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| libp2p flakiness / port conflicts | Medium | High | Deterministic port allocator, retries, serialize Tier‚ÄØ1 tests initially. (*Serialization still pending.) |
| Tier‚ÄØ1 suite too slow for per-PR | High | Medium | Keep Tier‚ÄØ0 for fast checks; run Tier‚ÄØ1 nightly until optimized. (Currently gated manually; `#[ignore]`/`serial` still TODO.) |
| Divergence between tiers | Low | Medium | Document scenario mapping; periodically run Tier‚ÄØ1 locally before releases. |
| Execution bridge complexity | Medium | High | Use HTTP stub first; defer real reth integration until harness stable. |
| Maintenance overhead | Medium | Medium | Share helpers between tiers, document setup, automate teardown. |

---

## 7. Notes

- **Tier strategy**: Tier‚ÄØ0 = state-level tests (`blob_state/`), Tier‚ÄØ1 = full-node tests (`full_node/`) with multi-validator topologies (three validators for quorum flow; four validators for ValueSync-only recovery such as `full_node_validator_restart_recovers`). Tier‚ÄØ0 stays default for `make itest`, Tier‚ÄØ1 becomes `make itest-node`.
- **Harness builder**: Tier‚ÄØ1 now exposes `FullNodeTestBuilder` in `crates/test/tests/full_node/node_harness.rs`, so scenarios describe only the consensus actions while setup/teardown stays centralized.
- **Payload planning**: The builder can inject a per-height blob schedule into the Engine stub, enabling blobless heights and multi-blob rounds (needed for the multi-height ValueSync restart scenario and future negative-path work).
- **State inspection helpers**: The harness can now reopen a validator‚Äôs stores after a clean shutdown, rebuild blob sidecars, and run `State::process_synced_package` directly. This powers the restart hydration, restream cleanup, and commitment-mismatch tests without hand-editing RocksDB.
- **Execution cadence**: `make itest-node` runs each Tier‚ÄØ1 test in its own `cargo test` process (`blob_quorum`, `validator_restart` (4-node ValueSync recovery), `restart_mid_height`, `new_node_sync`, `multi_height_valuesync_restart`, `restart_multi_height_rebuilds`, `restream_multiple_rounds_cleanup`, `restream_multi_validator`, `value_sync_commitment_mismatch`, `value_sync_inclusion_proof_failure`, `blob_blobless_sequence_behaves`, `blob_pruning_retains_recent_heights`, `sync_package_roundtrip`, `value_sync_proof_failure`) to avoid cross-test resource leaks. Running `cargo test -p ultramarine-test --test full_node -- --ignored` is fine for ad-hoc runs but may time out when chaining all scenarios inside one process.
- **Tier‚Äë0 ‚Üí Tier‚Äë1 migration**: We now have a broader set of promotions:  
  1. `blob_new_node_sync` ‚Üí `full_node_new_node_sync` (4-validator cluster, validator‚ÄØ3 rejoins after heights‚ÄØ1‚Äì2 and ValueSync fetches blobs/metadata).  
  2. `blob_restart_multi_height_sync` ‚Üí `full_node_multi_height_valuesync_restart` (validator‚ÄØ3 misses heights‚ÄØ1‚Äì3 with a 1/0/2 blob mix, ValueSync imports them, and we restart to assert store + parent-root hydration).  
  3. `blob_restart_multi_height` ‚Üí `full_node_restart_multi_height_rebuilds` (multi-height blob mix without ValueSync; restarts rely purely on on-disk metadata + blob store).  
  4. `blob_restream_multiple_rounds` ‚Üí `full_node_restream_multiple_rounds_cleanup` (uses real stores/keys to drive two rounds and ensure losing-round blobs are dropped at commit).  
  5. `blob_sync_commitment_mismatch_rejected` ‚Üí `full_node_value_sync_commitment_mismatch` (feeds a tampered ValueSync package through a full-node state to verify rejection + cleanup).  
  6. `blob_restream_multi_validator` ‚Üí `full_node_restream_multi_validator` (proposer/follower restream over real channels to confirm metrics/import paths).  
  7. `blob_sync_inclusion_proof_failure_rejected` ‚Üí `full_node_value_sync_inclusion_proof_failure` (corrupt inclusion proofs inside ValueSync packages and assert full-node rejection/cleanup).  
  8. `blob_blobless_sequence_behaves` ‚Üí `full_node_blob_blobless_sequence_behaves` (exercise mixed blob/bless heights with real metrics + storage).  
  9. `blob_pruning_retains_recent_heights` ‚Üí `full_node_blob_pruning_retains_recent_heights` (override retention window and ensure pruning/metrics behave with real stores).  
  10. `sync_package_roundtrip` ‚Üí `full_node_sync_package_roundtrip` (ingest a full ValueSync package and commit it end-to-end).  
  11. `blob_sync_failure_rejects_invalid_proof` ‚Üí `full_node_value_sync_proof_failure` (tamper blob proofs to cover the last ValueSync rejection path).  
  These scenarios prove the builder can handle restart, restream, and sync-negative paths without hand-editing RocksDB.
- **Architecture references**: Malachite‚Äôs `TestBuilder` (networked validators + followers; see `malachite/code/crates/test/tests/it/full_nodes.rs`) and Snapchain‚Äôs consensus harness (`snapchain/tests/consensus_test.rs`) are the baselines we mirror.
- **Next steps**: With the Tier‚Äë0 backlog promoted, the remaining work is wiring the builder to the execution bridge (real EngineClient / payload status plumbing) and deciding how to run those heavier cases in CI (Phase‚ÄØP5).

## 8. Current Status (Nov‚ÄØ2025)

- Tier‚ÄØ1 now ships fourteen scenarios: `full_node_blob_quorum_roundtrip`, `full_node_validator_restart_recovers`, `full_node_restart_mid_height`, `full_node_new_node_sync`, `full_node_multi_height_valuesync_restart`, `full_node_restart_multi_height_rebuilds`, `full_node_restream_multiple_rounds_cleanup`, `full_node_restream_multi_validator`, `full_node_value_sync_commitment_mismatch`, `full_node_value_sync_inclusion_proof_failure`, `full_node_blob_blobless_sequence_behaves`, `full_node_blob_pruning_retains_recent_heights`, `full_node_sync_package_roundtrip`, and `full_node_value_sync_proof_failure`. Together they cover quorum flow, restart hydration (with and without ValueSync), restream cleanup (multi-round + multi-validator), blobless/pruning behavior, and every ValueSync rejection path using the production state/engine wiring.
- **Known limitation for P2:** the remaining negative-path coverage (e.g., inclusion-proof failures) still lives only in Tier‚ÄØ0. With the builder now resuming height from disk and keeping the Engine stub head persistent across restarts, porting those cases plus additional restream permutations is unblocked‚ÄîThe restart regression (`full_node_validator_restart_recovers`) is now stable and deterministic (4-validator cluster with immediate node-0 shutdown forcing ValueSync), completing P2 requirements.

---

## 9. 2025‚Äë11 Review Findings

**Reference baselines**
- Snapchain‚Äôs consensus suite (`snapchain/tests/consensus_test.rs`) uses a deterministic builder with `serial_test`, explicit port allocation, and asserts on block store / shard store state rather than raw log output.
- Malachite‚Äôs `TestBuilder` scenarios (`malachite/code/crates/starknet/test/src/tests/full_nodes.rs`) always run ‚â•‚ÄØ3 validators plus followers, drive crash/restart/value-sync cases, and rely on the engine itself to report progress (`wait_until(height)` and `run_with_params(...)` manage deadlines/diagnostics).

**Tier‚ÄØ0 (state-level) gaps**
- No serialization: every `blob_state` test can run concurrently even though they share the same global trusted setup cache and each spins RocksDB in `/tmp`. This differs from Snapchain/Malachite where heavy cases are `#[serial]` or moved to a `make itest` target. Consequence: nondeterministic timing, high CPU, and noisy failures when multiple suites run together.
- Coverage stops at `State::process_decided_certificate`: none of the Tier‚ÄØ0 scenarios force Malachite to emit `Decided`, so bugs between votes and commit (e.g., missing POLC, WAL replay divergence) slip through. Reference suites enforce consensus-level assertions before calling into application state.
- Helpers trigger permanent compiler warnings (`tests/common/mod.rs` exports unused mocks, harness structs). Keeping unused scaffolding diverges from the cleaner shared modules in Snapchain/Malachite and signals duplication across tests.

**Tier‚ÄØ1 (full-node) gaps**
- Tests are neither ignored nor serialized (`node_harness.rs:87-120`). Running `cargo test -p ultramarine-test` attempts to boot libp2p + Engine stubs alongside the fast Tier‚ÄØ0 suite, causing port conflicts that reference harnesses avoid by gating (snapchain) or using `serial_test` (malachite).
- The harness mutates node state out-of-band (`initialize_genesis_metadata()` opens RocksDB directly before the app starts). Production nodes already seed via `State::hydrate_blob_parent_root()`; touching the DB externally bypasses WAL replay and can leave consensus believing it still owes height‚ÄØ0, which explains the missing `Decided` events we observed.
- Instrumentation is minimal: when `wait_for_height` times out we only see the last 10 broadcast events. Reference harnesses inspect block stores, WAL entries, and consensus metrics to emit actionable reasons (‚ÄúPOLC not reached‚Äù, ‚Äúvalue sync pending‚Äù). We should expose similar checks (e.g., query `Store::get_decided_value` per node, dump WAL).
- Engine stub closes the TCP connection after each request (`node_harness.rs:462-520`), whereas real Engine API connections stay open. Hyper retries hide the error, but repeated connection resets slow tests and obscure root causes; Snapchain/Malachite stubs keep connections alive and handle multiple RPCs per session.
- Every node is forced to `start_height: Some(Height::new(1))` with empty stores. Malachite‚Äôs builder either populates genesis in advance or lets the node derive start height from disk. Our shortcut means consensus believes it skipped height‚ÄØ0 even though the WAL is empty, leading to the stalled round‚Äë1 behaviour currently seen.

**Recommended remediation**
1. Gate Tier‚ÄØ1 behind `#[ignore]` + `make itest-node`, add `serial_test::serial` so only one harness manipulates ports/state at a time.
2. Remove the manual RocksDB seeding from the harness and let `App::start` seed genesis metadata. If we must pre-populate, spawn the app once to height‚ÄØ0 rather than editing the DB externally.
3. Upgrade instrumentation: ‚úÖ Store snapshot + WAL tail now dump automatically when a node times out. Future work: expose WAL decoding in a helper so we can assert on specific entry types.
4. Extend Tier‚ÄØ0 to cover proposer restreams and WAL replay by driving multiple rounds within the same test (serialised). This plugs the ‚ÄúDecided never called‚Äù gap without waiting for Tier‚ÄØ1 to finish.
5. Adopt a builder DSL (either port Malachite‚Äôs `TestBuilder` or create a thin wrapper) so future scenarios describe validator/full-node lifecycles declaratively, matching our reference implementations and reducing bespoke harness code.
